{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook is copied from Phung's Kaggle notebook 'Pytorch BERT for Named Entity Recognition'. Changes are marked in __*italic bold*__.\n\nThis notebook shows how to fine-tune a BERT model (from huggingface) for our dataset recognition task.\n\nNote that internet is needed during the training phase (for downloading the bert-base-cased model). Internet can be turned off during prediction.","metadata":{}},{"cell_type":"markdown","source":"## Install packages","metadata":{}},{"cell_type":"code","source":"!pip install datasets --no-index --find-links=file:///kaggle/input/coleridge-packages/packages/datasets\n!pip install ../input/coleridge-packages/seqeval-1.2.2-py3-none-any.whl\n!pip install ../input/coleridge-packages/tokenizers-0.10.1-cp37-cp37m-manylinux1_x86_64.whl\n!pip install ../input/coleridge-packages/transformers-4.5.0.dev0-py3-none-any.whl","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import","metadata":{}},{"cell_type":"code","source":"import os\nimport re\nimport json\nimport time\nimport datetime\nimport random\nimport glob\nimport importlib\n\nimport numpy as np\nimport pandas as pd\n\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nrandom.seed(123)\nnp.random.seed(456)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T22:51:18.836163Z","iopub.execute_input":"2021-05-30T22:51:18.836595Z","iopub.status.idle":"2021-05-30T22:51:19.686375Z","shell.execute_reply.started":"2021-05-30T22:51:18.836499Z","shell.execute_reply":"2021-05-30T22:51:19.685301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# copy my_seqeval.py to the working directory because the input directory is non-writable\n!cp /kaggle/input/coleridge-packages/my_seqeval.py ./","metadata":{"execution":{"iopub.status.busy":"2021-05-30T22:51:19.688788Z","iopub.execute_input":"2021-05-30T22:51:19.689123Z","iopub.status.idle":"2021-05-30T22:51:20.432209Z","shell.execute_reply.started":"2021-05-30T22:51:19.689095Z","shell.execute_reply":"2021-05-30T22:51:20.430793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hyper-parameters","metadata":{}},{"cell_type":"code","source":"MAX_LENGTH = 64 # max no. words for each sentence.\nOVERLAP = 20 # if a sentence exceeds MAX_LENGTH, we split it to multiple sentences with overlapping\n\nMAX_SAMPLE = None # set a small number for experimentation, set None for production.","metadata":{"execution":{"iopub.status.busy":"2021-05-30T22:51:20.43409Z","iopub.execute_input":"2021-05-30T22:51:20.434428Z","iopub.status.idle":"2021-05-30T22:51:20.439981Z","shell.execute_reply.started":"2021-05-30T22:51:20.434399Z","shell.execute_reply":"2021-05-30T22:51:20.438701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load data","metadata":{}},{"cell_type":"code","source":"train_path = '../input/coleridgeinitiative-show-us-the-data/train.csv'\npaper_train_folder = '../input/coleridgeinitiative-show-us-the-data/train'\n\ntrain = pd.read_csv(train_path)\ntrain = train[:MAX_SAMPLE]\nprint(f'No. raw training rows: {len(train)}')","metadata":{"execution":{"iopub.status.busy":"2021-05-30T22:51:20.441193Z","iopub.execute_input":"2021-05-30T22:51:20.441674Z","iopub.status.idle":"2021-05-30T22:51:20.599255Z","shell.execute_reply.started":"2021-05-30T22:51:20.441645Z","shell.execute_reply":"2021-05-30T22:51:20.598065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Group by publication, training labels should have the same form as expected output.","metadata":{}},{"cell_type":"code","source":"train = train.groupby('Id').agg({\n    'pub_title': 'first',\n    'dataset_title': '|'.join,\n    'dataset_label': '|'.join,\n    'cleaned_label': '|'.join\n}).reset_index()\n\nprint(f'No. grouped training rows: {len(train)}')","metadata":{"execution":{"iopub.status.busy":"2021-05-30T22:51:20.601216Z","iopub.execute_input":"2021-05-30T22:51:20.601494Z","iopub.status.idle":"2021-05-30T22:51:21.278724Z","shell.execute_reply.started":"2021-05-30T22:51:20.601468Z","shell.execute_reply":"2021-05-30T22:51:21.277556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"papers = {}\nfor paper_id in train['Id'].unique():\n    with open(f'{paper_train_folder}/{paper_id}.json', 'r') as f:\n        paper = json.load(f)\n        papers[paper_id] = paper","metadata":{"execution":{"iopub.status.busy":"2021-05-30T22:51:21.28045Z","iopub.execute_input":"2021-05-30T22:51:21.280743Z","iopub.status.idle":"2021-05-30T22:52:25.004025Z","shell.execute_reply.started":"2021-05-30T22:51:21.280716Z","shell.execute_reply":"2021-05-30T22:52:25.002675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"_**Load gov dataset and randomly shuffle.**_","metadata":{}},{"cell_type":"code","source":"gov_dataDF = pd.read_csv('../input/gov-data/additional_gov_datasets_popular.csv').drop_duplicates()\ngov_dataDF.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-30T23:01:40.714874Z","iopub.execute_input":"2021-05-30T23:01:40.715256Z","iopub.status.idle":"2021-05-30T23:01:41.369878Z","shell.execute_reply.started":"2021-05-30T23:01:40.715222Z","shell.execute_reply":"2021-05-30T23:01:41.36882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gov_data = [item for sublist in [list(row) for row in gov_dataDF.values] for item in sublist]\nrandom.shuffle(gov_data)\ngov_data[:5]","metadata":{"execution":{"iopub.status.busy":"2021-05-30T23:05:54.469578Z","iopub.execute_input":"2021-05-30T23:05:54.470102Z","iopub.status.idle":"2021-05-30T23:05:55.128992Z","shell.execute_reply.started":"2021-05-30T23:05:54.470056Z","shell.execute_reply":"2021-05-30T23:05:55.127845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(gov_data)","metadata":{"execution":{"iopub.status.busy":"2021-05-30T23:05:51.816879Z","iopub.execute_input":"2021-05-30T23:05:51.817456Z","iopub.status.idle":"2021-05-30T23:05:51.822932Z","shell.execute_reply.started":"2021-05-30T23:05:51.817423Z","shell.execute_reply":"2021-05-30T23:05:51.821984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Transform data","metadata":{}},{"cell_type":"code","source":"def clean_training_text(txt):\n    \"\"\"\n    similar to the default clean_text function but without lowercasing.\n    \"\"\"\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt)).strip()\n\ndef shorten_sentences(sentences):\n    short_sentences = []\n    for sentence in sentences:\n        words = sentence.split()\n        if len(words) > MAX_LENGTH:\n            for p in range(0, len(words), MAX_LENGTH - OVERLAP):\n                short_sentences.append(' '.join(words[p:p+MAX_LENGTH]))\n        else:\n            short_sentences.append(sentence)\n    return short_sentences\n\ndef find_sublist(big_list, small_list):\n    all_positions = []\n    for i in range(len(big_list) - len(small_list) + 1):\n        if small_list == big_list[i:i+len(small_list)]:\n            all_positions.append(i)\n    \n    return all_positions\n\ndef tag_sentence(sentence, labels): # requirement: both sentence and labels are already cleaned\n    sentence_words = sentence.split()\n    \n    if labels is not None and any(re.findall(f'\\\\b{label}\\\\b', sentence) for label in labels): # positive sample\n        nes = ['O'] * len(sentence_words)\n        for label in labels:\n            label_words = label.split()\n\n            all_pos = find_sublist(sentence_words, label_words)\n            for pos in all_pos:\n                nes[pos] = 'B'\n                for i in range(pos+1, pos+len(label_words)):\n                    nes[i] = 'I'\n\n        return list(zip(sentence_words, nes))\n        \n    else: # negative sample\n        nes = ['O'] * len(sentence_words)\n        return list(zip(sentence_words, nes))","metadata":{"execution":{"iopub.status.busy":"2021-05-30T23:02:04.649851Z","iopub.execute_input":"2021-05-30T23:02:04.650325Z","iopub.status.idle":"2021-05-30T23:02:04.670813Z","shell.execute_reply.started":"2021-05-30T23:02:04.65028Z","shell.execute_reply":"2021-05-30T23:02:04.669829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"__*Create the training data.*__\n* _tagged_list1_ consists of positive sentences with a dataset label, filtered that they do not contain any other capital letters (except for the beginning of the sentence). \n* _tagged_list2_ consists of negative sentences containing the words ‘data’ and ‘study’, filtered that they do not contain any other capital letters (except for the beginning of the sentence).\n* _tagged_list3_ consists of negative sentences not containing the words ‘data’ and ‘study’, but with capital letters.\n* _tagged_list4_ consists of positive sentences taken from the _tagged_list1_ in which the dataset label is replaced by one of the external GOV dataset labels.\n\n__*Variations in methods are the following:*__\n* The list with 'data' and 'study' are expanded with 'project', 'program' and 'survey'.\n* _tagged_list1_ consists of all positive sentences, not filtered on capital letters (code for this can be found in Phung's original notebook).\n* _tagged_list2_ consists of all negative sentences containing the words 'data' and 'study, not filtered on capital letters (code for this can be found in Phung's original notebook).","metadata":{}},{"cell_type":"code","source":"tagged_list1 = []\ntagged_list2 = []\ntagged_list3 = []\ntagged_list4 = []\ncntNO, cnt1L, cntYES, cntGOV = 0, 0, 0, 0\n\npbar = tqdm(total=len(train))\nfor i, id, dataset_label in train[['Id', 'dataset_label']].itertuples():\n    # paper\n    paper = papers[id]\n    \n    # labels\n    labels = dataset_label.split('|')\n    labels = [clean_training_text(label) for label in labels]\n    \n    # sentences\n    sentences = set([clean_training_text(sentence) for section in paper \n                 for sentence in section['text'].split('.')\n                ])\n    sentences = shorten_sentences(sentences) # make sentences short\n    sentences = [sentence for sentence in sentences if len(sentence) > 10] # only accept sentences with length > 10 chars\n    \n    # for each sentence\n    for sentence in sentences:\n        sentence_words = sentence.split()\n        \n        # sentences with label\n        if labels is not None and any(re.findall(f'\\\\b{label}\\\\b', sentence) for label in labels): # find all positive samples           \n            # sentences with label and no other capitals, most likely to not have unknown dataset labels\n            for label in labels:\n                sentence2 = sentence.replace(label, label.lower()) # replace dataset label by lowercase to recognize other capitals\n                sentence3 = sentence2[0].lower() + sentence2[1:] # do not take first character of sentence into account\n                                \n                if sentence3.islower():\n                    cnt1L+=1\n                    tagged_list1.append(tag_sentence(sentence, labels))\n                    \n                    # add sentences with another dataset from external dataset\n                    if cnt1L*1 < len(gov_data)-1:\n                        cntGOV+=1\n                        sentenceNew1 = sentence.replace(label, gov_data[cnt1L*1])\n                        tagged_list4.append(tag_sentence(sentenceNew1, [gov_data[cnt1L*1]]))\n                    if cnt1L*2 < len(gov_data)-1:\n                        cntGOV+=1\n                        sentenceNew2 = sentence.replace(label, gov_data[cnt1L*2])\n                        tagged_list4.append(tag_sentence(sentenceNew2, [gov_data[cnt1L*2]]))\n                    if cnt1L*3 < len(gov_data)-1:\n                        cntGOV+=1\n                        sentenceNew3 = sentence.replace(label, gov_data[cnt1L*3])\n                        tagged_list4.append(tag_sentence(sentenceNew3, [gov_data[cnt1L*3]]))\n                    \n        # sentences with no capitals when data or study is mentioned, most likely to not have unknown dataset labels\n        if any(word in sentence.lower() for word in ['data', 'study']):\n            sentenceL = sentence[0].lower() + sentence[1:] # do not take first character of sentence into account\n            if sentenceL.islower():\n                cntNO+=1\n                tagged_list2.append(tag_sentence(sentence, labels))\n                \n        # sentences with capitals but not including data or study\n        if not any(word in sentence.lower() for word in ['data', 'study']): \n            sentenceL = sentence[0].lower() + sentence[1:]\n            if not sentenceL.islower():\n                cntYES+=1\n                tagged_list3.append(tag_sentence(sentence, labels))\n    \n    # process bar\n    pbar.update(1)\n    pbar.set_description(f\"Training data size: {cnt1L} one label, {cntNO} no label, {cntYES} basic, {cntGOV} gov label\")","metadata":{"execution":{"iopub.status.busy":"2021-05-30T23:07:29.578072Z","iopub.execute_input":"2021-05-30T23:07:29.578747Z","iopub.status.idle":"2021-05-30T23:07:43.134901Z","shell.execute_reply.started":"2021-05-30T23:07:29.578708Z","shell.execute_reply":"2021-05-30T23:07:43.132852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"__*Merge the tagged_lists together with desired sizes (differs per method).*__","metadata":{}},{"cell_type":"code","source":"tagged_list = tagged_list1 + random.sample(tagged_list2, int(len(tagged_list2)/2)) + random.sample(tagged_list3, int(len(tagged_list2)/2)) + tagged_list4\n\n#shuffling\nrandom.shuffle(tagged_list)","metadata":{"execution":{"iopub.status.busy":"2021-05-28T12:59:01.404706Z","iopub.execute_input":"2021-05-28T12:59:01.405112Z","iopub.status.idle":"2021-05-28T12:59:01.432524Z","shell.execute_reply.started":"2021-05-28T12:59:01.405079Z","shell.execute_reply":"2021-05-28T12:59:01.431368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(tagged_list1))\nprint(len(tagged_list2))\nprint(len(tagged_list3))\nprint(len(tagged_list4))\nprint(len(tagged_list))","metadata":{"execution":{"iopub.status.busy":"2021-05-28T12:59:04.705548Z","iopub.execute_input":"2021-05-28T12:59:04.706093Z","iopub.status.idle":"2021-05-28T12:59:04.712129Z","shell.execute_reply.started":"2021-05-28T12:59:04.706061Z","shell.execute_reply":"2021-05-28T12:59:04.710882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(tagged_list[:5])","metadata":{"execution":{"iopub.status.busy":"2021-05-28T12:59:56.252428Z","iopub.execute_input":"2021-05-28T12:59:56.252826Z","iopub.status.idle":"2021-05-28T12:59:56.257681Z","shell.execute_reply.started":"2021-05-28T12:59:56.252789Z","shell.execute_reply":"2021-05-28T12:59:56.256861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"write data to file.","metadata":{}},{"cell_type":"code","source":"with open('train_ner.json', 'w') as f:\n    for row in tagged_list:\n        words, nes = list(zip(*row))\n        row_json = {'tokens' : words, 'tags' : nes}\n        json.dump(row_json, f)\n        f.write('\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fine-tune a BERT model for NER","metadata":{}},{"cell_type":"code","source":"!python ../input/kaggle-ner-utils/kaggle_run_ner.py \\\n--model_name_or_path 'bert-base-cased' \\\n--train_file './train_ner.json' \\\n--validation_file './train_ner.json' \\\n--num_train_epochs 1 \\\n--per_device_train_batch_size 8 \\\n--per_device_eval_batch_size 8 \\\n--save_steps 15000 \\\n--output_dir './output' \\\n--report_to 'none' \\\n--seed 123 \\\n--do_train ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After the tuning finishes, we should find our model in './output'.","metadata":{}}]}